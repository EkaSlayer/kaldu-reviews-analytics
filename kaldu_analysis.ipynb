{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6c93cd1",
   "metadata": {},
   "source": [
    "# Kaldu Reviews Analytics (Public/Non-confidential Data)\n",
    "**Notebook**: End-to-end EDA, sentiment analysis, and topic cues.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64803690",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np, re, string, json\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_excel(\"../data/kaldu-cleaning.xlsx\", sheet_name=0).copy()\n",
    "df = df.drop_duplicates(subset=[\"author\", \"timestamp\", \"text\"]).reset_index(drop=True)\n",
    "\n",
    "def try_parse_datetime(x):\n",
    "    try:\n",
    "        return pd.to_datetime(x)\n",
    "    except Exception:\n",
    "        return pd.NaT\n",
    "df[\"timestamp_parsed\"] = df[\"timestamp\"].apply(try_parse_datetime)\n",
    "\n",
    "df[\"text\"] = df[\"text\"].fillna(\"\")\n",
    "df[\"owner_reply\"] = df[\"owner_reply\"].fillna(\"\")\n",
    "df[\"has_owner_reply\"] = df[\"owner_reply\"].str.strip().ne(\"\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ad7e31",
   "metadata": {},
   "source": [
    "## Sentiment Analysis\n",
    "We use VADER if available, otherwise a tiny Indonesian/English lexicon fallback (illustrative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375a137e",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_vader = False\n",
    "scores = None\n",
    "try:\n",
    "    from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "    import nltk\n",
    "    try:\n",
    "        _ = nltk.data.find('sentiment/vader_lexicon.zip')\n",
    "    except LookupError:\n",
    "        nltk.download('vader_lexicon', quiet=True)\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    use_vader = True\n",
    "except Exception as e:\n",
    "    use_vader = False\n",
    "\n",
    "pos_words = set(\"\"\"mantap enak lezat nikmat recommended cepat ramah bersih murah terbaik puas suka love great good awesome nice delicious tasty friendly clean cheap fast amazing fresh helpful nyaman cepat\"\"\".split())\n",
    "neg_words = set(\"\"\"buruk lama mahal tidak enak basi mengecewakan kotor lambat parah jelek pahit asin asam overprice overprized overpricey wait cold rude raw undercooked oily hambar\"\"\".split())\n",
    "\n",
    "import re\n",
    "def simple_sentiment_score(text):\n",
    "    tokens = re.findall(r\"\\w+\", str(text).lower())\n",
    "    if not tokens:\n",
    "        return 0.0\n",
    "    score = sum(1 for t in tokens if t in pos_words) - sum(1 for t in tokens if t in neg_words)\n",
    "    return score / max(len(tokens), 1)\n",
    "\n",
    "if use_vader:\n",
    "    df[\"sentiment\"] = df[\"text\"].apply(lambda t: sia.polarity_scores(str(t))[\"compound\"])\n",
    "else:\n",
    "    df[\"sentiment\"] = df[\"text\"].apply(simple_sentiment_score)\n",
    "\n",
    "def to_label(s):\n",
    "    if s >= 0.2: \n",
    "        return \"positive\"\n",
    "    elif s <= -0.2:\n",
    "        return \"negative\"\n",
    "    else:\n",
    "        return \"neutral\"\n",
    "df[\"sentiment_label\"] = df[\"sentiment\"].apply(to_label)\n",
    "df[\"sentiment_label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a100e4c3",
   "metadata": {},
   "source": [
    "## N-grams and Topic Cues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a23fdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(t):\n",
    "    t = str(t).lower()\n",
    "    t = re.sub(r\"http\\S+\", \" \", t)\n",
    "    import string\n",
    "    t = t.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    t = re.sub(r\"\\d+\", \" \", t)\n",
    "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
    "    return t\n",
    "\n",
    "df[\"text_norm\"] = df[\"text\"].apply(normalize_text)\n",
    "\n",
    "stop_id = set(\"\"\"yang dan di ke dari untuk pada dengan ini itu itu nya si nya nya lah kok dong deh kan juga kita kamu saya saya nya tidak bukan ya sudah aja biar agar kalau karena jadi saat oleh dalam mereka dia nya para para para para\"\"\".split())\n",
    "stop_en = set(\"\"\"the a an and or of in on at to for from as is are was were be been being by with this that it its you your we our they their them he she his her i me my mine\"\"\".split())\n",
    "stop = list(stop_id | stop_en)\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=3, max_df=0.9, ngram_range=(1,2), stop_words=stop)\n",
    "X_counts = vectorizer.fit_transform(df[\"text_norm\"])\n",
    "vocab = vectorizer.get_feature_names_out()\n",
    "sum_counts = X_counts.sum(axis=0).A1\n",
    "top_idx = sum_counts.argsort()[::-1][:20]\n",
    "top_terms = [(vocab[i], int(sum_counts[i])) for i in top_idx]\n",
    "top_terms[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb90538",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Reviews per month\n",
    "if df[\"timestamp_parsed\"].notna().any():\n",
    "    df[\"month\"] = df[\"timestamp_parsed\"].dt.to_period(\"M\").astype(str)\n",
    "else:\n",
    "    df[\"month\"] = \"unknown\"\n",
    "\n",
    "reviews_per_month = df.groupby(\"month\").size().reset_index(name=\"reviews\")\n",
    "reviews_per_month = reviews_per_month.sort_values(\"month\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(reviews_per_month[\"month\"], reviews_per_month[\"reviews\"], marker=\"o\")\n",
    "plt.title(\"Reviews per Month\")\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Sentiment distribution\n",
    "sentiment_share = df[\"sentiment_label\"].value_counts().reindex([\"positive\",\"neutral\",\"negative\"]).fillna(0).astype(int)\n",
    "plt.figure()\n",
    "sentiment_share.plot(kind=\"bar\")\n",
    "plt.title(\"Sentiment Distribution\")\n",
    "plt.xlabel(\"Sentiment\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Top terms bar\n",
    "terms, counts = zip(*top_terms) if top_terms else ([], [])\n",
    "plt.figure()\n",
    "plt.barh(range(len(terms))[::-1], list(counts)[::-1])\n",
    "plt.yticks(range(len(terms))[::-1], list(terms)[::-1])\n",
    "plt.title(\"Top Terms / N-grams\")\n",
    "plt.xlabel(\"Frequency\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d40acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple topics via KMeans (unsupervised)\n",
    "topics = []\n",
    "if X_counts.shape[0] >= 20 and X_counts.shape[1] >= 10:\n",
    "    k = 5 if X_counts.shape[0] > 100 else 3\n",
    "    km = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    labels = km.fit_predict(X_counts)\n",
    "    df[\"topic\"] = labels\n",
    "    centers = km.cluster_centers_\n",
    "    for i in range(k):\n",
    "        top_terms_idx = centers[i].argsort()[::-1][:8]\n",
    "        topics.append({\"topic_id\": i, \"top_terms\": [vocab[j] for j in top_terms_idx], \"size\": int((labels == i).sum())})\n",
    "topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826a50e7",
   "metadata": {},
   "source": [
    "## Owner Reply Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c790b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "owner_reply_rate = df[\"has_owner_reply\"].mean() if len(df)>0 else 0.0\n",
    "owner_reply_rate"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
